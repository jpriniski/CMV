{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this iPython Notebook, we will connect to the Reddit API, develop a set of methods that determines rates of delta awarding and use of evidence in comments per discussion. This is the notebook that collected and analyzed data for the paper: Priniski, H. J., & Horne, Z. (Under review). Attitude Change on Reddit's Change My View. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 5.1.0 of praw is outdated. Version 5.3.0 was released Sunday December 17, 2017.\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(client_id='',\n",
    "                     client_secret='',\n",
    "                     password='',\n",
    "                     user_agent='',\n",
    "                     username='')\n",
    "\n",
    "\n",
    "subreddit = reddit.subreddit('changemyview')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting deltas and delta awarded comments\n",
    "The block below has functions that distill delta awarded comments and their subsequent threads of discussion.  This is done using breadth-first search through the dicsussion tree returned by the Reddit API.  To return a delta thread, the tree is traversed upwards until the root of the thread is found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_delta(body):\n",
    "    if 'confirmed: 1 delta awarded to' in body.lower():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def set_value(value, value_list):\n",
    "    value_list.append(value)\n",
    "    return\n",
    "\n",
    "def get_thread(comment, root, thread):\n",
    "\n",
    "    if comment.parent() is root:\n",
    "        return set_value(comment.body, thread)\n",
    "    get_thread(comment.parent(), root, thread)\n",
    "    set_value(comment.body, thread)\n",
    "\n",
    "def get_delta_thread(post):\n",
    "    post.comments.replace_more(limit = 0)\n",
    "    queue = post.comments[:]\n",
    "    threads = []\n",
    "    root = post\n",
    "    while queue:\n",
    "        comment = queue.pop(0)\n",
    "        if has_delta(comment.body):\n",
    "            thread = []\n",
    "            get_thread(comment, root, thread)\n",
    "            threads.append(thread)\n",
    "        queue.extend(comment.replies)\n",
    "    \n",
    "    return threads\n",
    "\n",
    "def get_delta_count(comments):\n",
    "    count = 0\n",
    "    for comment in comments:\n",
    "        if has_delta(comment):\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyze_data exectues the majority of this notebook's functionality\n",
    "Not all of the variables we wish to explore are returned by the Reddit API.  Therefore, we have to implement our own functions to find the number of deltas awarded in each discussion, distill the threads of discussion that lead to a delta, the number of links used in a discussion, etc.  The block of code below executes this functionality.  Since this function isn't very modularized and easy to follow, I descibe the steps of the function here. \n",
    "\n",
    "We will pass this function a post object.  It is a rich data structure returned by the Reddit API. \n",
    "```python\n",
    "def analyze_data(post):\n",
    "```\n",
    "\n",
    "The following variables will hold the information:\n",
    "- `comments`: a compressed list of the post's discussion\n",
    "- `threads`: a list of the threads of conversation that lead to a delta being awarded\n",
    "- `DACs`: a dictionary of the delta awarded comments and the amount of deltas awarded to it\n",
    "- `links`: a list of links used in the discussion (len(links) returns the amount of links used in the discussion)\n",
    "- `stat_lang_general`: the amount of posts that use statistically-oriented language used in the discussion \n",
    "- `stat_lang_deltas`: the amount delta awarded comments that use statistically-oriented langauage\n",
    "\n",
    "\n",
    "```python\n",
    "    comments = []\n",
    "    threads = []\n",
    "    DACs = dict()\n",
    "    links = []\n",
    "    #number of posts in the discussion using statistical language\n",
    "    stat_lang_general = 0\n",
    "    #number of DACs in the discussion using statistical language\n",
    "    stat_lang_deltas = dict()\n",
    "    \n",
    "```\n",
    "\n",
    "We must treet the comments object returned by the Reddit API as a tree. Therefore, conventional tree traversal methods are used  to go through the comment.  To do this, we treat the tree like data structure like a queue of posts.  \n",
    "\n",
    "```python\n",
    "    com = post.comments.replace_more(limit = 0)\n",
    "    com_tree = post.comments[:]\n",
    "    root = post\n",
    "    while com_tree:\n",
    "        comment = com_tree.pop(0)\n",
    "```\n",
    "We check for links, use of statistical language and if it contains a delta right away when when we first encounter a new comment\n",
    "```python\n",
    "        links.append([w for w in comment.body.lower().split() if has_link(w)])\n",
    "        \n",
    "        if has_stat_language(comment.body.lower()):\n",
    "            stat_lang_general += 1\n",
    "        comments.append(comment.body.lower())\n",
    "        \n",
    "        if has_delta(comment.body):\n",
    "                \n",
    "            thread = []\n",
    "            get_thread(comment, root, thread)\n",
    "            threads.append(thread)\n",
    "            \n",
    "            #get the root comment, the DAC by indexing the first element in the list\n",
    "            if has_stat_language(thread[0]):\n",
    "                stat_lang_deltas[thread[0]] =1\n",
    "\n",
    "            if has_link(thread[0]):\n",
    "                DACs[thread[0]] = 1\n",
    "            else:\n",
    "                DACs[thread[0]] = 0\n",
    "                  \n",
    "        com_tree.extend(comment.replies)\n",
    "```\n",
    "We then return the variables listed above.\n",
    "```python\n",
    "    return (comments, threads, DACs, links, stat_lang_general, stat_lang_deltas)\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_data(post):\n",
    "    #post = reddit.submission(url = url)\n",
    "\n",
    "    comments = []\n",
    "    threads = []\n",
    "    DACs = dict()\n",
    "    links = []\n",
    "    \n",
    "    #number of posts in the discussion using statistical language\n",
    "    stat_lang_general = 0\n",
    "    \n",
    "    #number of DACs in the discussion using statistical language\n",
    "    stat_lang_deltas = dict()\n",
    "    \n",
    "    com = post.comments.replace_more(limit = 0)\n",
    "    com_tree = post.comments[:]\n",
    "    root = post\n",
    "    while com_tree:\n",
    "        comment = com_tree.pop(0)\n",
    "\n",
    "        #check for links here\n",
    "        links.append([w for w in comment.body.lower().split() if has_link(w)])\n",
    "        \n",
    "        if has_stat_language(comment.body.lower()):\n",
    "            stat_lang_general += 1\n",
    "        comments.append(comment.body.lower())\n",
    "        \n",
    "        if has_delta(comment.body):\n",
    "                \n",
    "            thread = []\n",
    "            get_thread(comment, root, thread)\n",
    "            threads.append(thread)\n",
    "            \n",
    "            #get the root comment, the DAC by indexing the first element in the list\n",
    "            if has_stat_language(thread[0]):\n",
    "                stat_lang_deltas[thread[0]] =1\n",
    "            \n",
    "            if has_link(thread[0]):\n",
    "                DACs[thread[0]] = 1\n",
    "            else:\n",
    "                DACs[thread[0]] = 0\n",
    "    \n",
    "                    \n",
    "        com_tree.extend(comment.replies)\n",
    "    return (comments, threads, DACs, links, stat_lang_general, stat_lang_deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the statistically-oriented language in each post.  These functions calcaulte if there are digits and commonly used statistical analysis words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_dig(w):\n",
    "    return any(ch.isdigit() for ch in w)\n",
    "\n",
    "def has_stat_language(w):\n",
    "    terms = ['data', 'stats', 'statistics', 'figures', '%', 'percent', 'average']\n",
    "    if any(t in w for t in terms) or has_dig(w):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We are also interested in how frequently people cite external data.  The functions below do two things: calculate the total number of links used in a discussions, and calcuate the total number of comments using a link. Additionally, we save what links are used so we can analyze the types of data repliers like to incoperate into their replies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_link(w):\n",
    "    extensions = ['http://', '.com', '.org', '.gov', 'pdf', '.net', 'www.']\n",
    "    if any(e in w for e in extensions) and not ('reddit.com' in w):\n",
    "        return True\n",
    "\n",
    "def cnt_links(links):\n",
    "    com_links = 0\n",
    "    tot_links = 0\n",
    "    links_list = []\n",
    "    for l in links:\n",
    "        if not (l == []):\n",
    "            post_links = []\n",
    "            for _ in l:\n",
    "                post_links.append(_)\n",
    "                tot_links +=1\n",
    "            links_list.append(post_links)\n",
    "            com_links += 1\n",
    "    return com_links, tot_links, links_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we collect posts and run the analyze_data function on each submission, a directory will be created to clearnly save all the relvant data in its proper subdirectories. \n",
    "\n",
    "To signify when the data is collected, the date variable will equal today's date. \n",
    "\n",
    "We are also are collecting top reddit posts. If you are interested in collecting a different type of post, e.g. 'hot' posts, then change the variable `posts` to hot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = dt.datetime.now()\n",
    "\n",
    "date = str(now.month) + '-' + str(now.day)\n",
    "posts = 'top'\n",
    "directory = 'data/'+date+'/delta_threads/'\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you will signfy how any posts you want to collect.  Then a directory will be created with the following information:\n",
    "\n",
    "The delta_threads directory is a directory of all the deltas awarded in the discussion.  You can find the post text and title of the each discussion (along with the number of deltas awarded, number of comments, number of links used, etc.) in the synopsis_data.xlsx spreadsheet.  The name of each `delta_thread` file follows the following order: `post-number_date-of-collection_delta_threads.xlsx`\n",
    "\n",
    "\n",
    "\n",
    "`date\n",
    "|----delta_threads\n",
    "|    |\n",
    "|    |-----01_date_delta_threads.xlsx, 02_date_delta_threads.xlsx, ...\n",
    "|\n",
    "|----date_synopsis_data.xlsx`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subreddit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5289b30e4ca6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mNO_OF_POSTS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m525\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubreddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNO_OF_POSTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpost_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subreddit' is not defined"
     ]
    }
   ],
   "source": [
    "df = []\n",
    "\n",
    "#post count will identify the post/submission number.  it is useful for comparing data in the synopsis sheet \n",
    "# to the data in the delta thread directories\n",
    "post_cnt = 0\n",
    "\n",
    "NO_OF_POSTS = 525\n",
    "\n",
    "for submission in subreddit.top(limit=NO_OF_POSTS):\n",
    "    \n",
    "    post_time = submission.created\n",
    "    title = submission.title\n",
    "    selftext = submission.selftext.replace(',',' ')\n",
    "    comments, threads, DACs, links, stat_lang_general, stat_lang_deltas = analyze_data(submission)\n",
    "    #you can save comments to a csv if you desire.  they are returned as a compressed list and don't have their \n",
    "    #tree encoding\n",
    "    delta_count = get_delta_count(comments)\n",
    "    x = sum(DACs.values())\n",
    "    df.append([title,selftext, len(comments), len(DACs.keys()), delta_count, cnt_links(links)[0], cnt_links(links)[1],cnt_links(links)[2],x,stat_lang_general, len(stat_lang_deltas.keys()), post_time])\n",
    "    \n",
    "    threads_DF = pd.DataFrame(threads) \n",
    "    threads_DF.to_excel(directory+str(post_cnt)+'_'+date+'_delta_thread.xlsx', header = False)\n",
    "    post_cnt += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more detailed description of the steps from the convoluted block of code above follows here:\n",
    "\n",
    "For convience, we will save our data to a list of lists data strucutre and later convert this to a pandas dataframe.  Here we also determine how many submissions we wish to collect.  In our case it's `525`.\n",
    "```python\n",
    "df = []\n",
    "post_cnt = 0\n",
    "NO_OF_POSTS = 525\n",
    "```\n",
    "\n",
    "To connect to the Reddit api we must call the `subreddit` object created above.  We pass this the number of posts we wish to collect.  \n",
    "\n",
    "```python\n",
    "for submission in subreddit.top(limit=NO_OF_POSTS):\n",
    "```\n",
    "\n",
    "A submission variable is created for each post, and we will iterate through as many posts equal to `NO_OF_POSTS`.\n",
    "\n",
    "The information we can collect directly from the Reddit API is\n",
    "\n",
    "```python\n",
    "    post_time = submission.created\n",
    "    title = submission.title\n",
    "    selftext = submission.selftext.replace(',',' ')\n",
    "```\n",
    "\n",
    "The rest of the variables we are interested in analyzing, such as the number of deltas, the delta threads, the use of links and statistically-oriented language needs to be calcaulted locally. We can do this by calling the `analyze_data` function implemented above.  \n",
    "```python\n",
    "    comments, threads, DACs, links, stat_lang_general, stat_lang_deltas = analyze_data(submission)\n",
    "    #Here we can save comments to a csv if you desire.  they are returned as a compressed list and don't have their tree encoding\n",
    "    delta_count = get_delta_count(comments)\n",
    "    x = sum(DACs.values())\n",
    "    \n",
    "```\n",
    "We will now append all of the relvant information to our list of lists `df`. This data will be saved the synopsis file. \n",
    "\n",
    "```python\n",
    "    df.append([title,selftext, len(comments), len(DACs.keys()), delta_count, cnt_links(links)[0], cnt_links(links)[1],cnt_links(links)[2],x,stat_lang_general, len(stat_lang_deltas.keys()), post_time])\n",
    "```\n",
    "\n",
    "All delta threads will be saved in it's own directory.  We signfy this directory here.\n",
    "\n",
    "```python\n",
    "    threads_DF = pd.DataFrame(threads) \n",
    "    threads_DF.to_excel(directory+str(post_cnt)+'_'+date+'_delta_thread.xlsx', header = False)\n",
    "```\n",
    "\n",
    "\n",
    "And of course, implement our post count \n",
    "```python\n",
    "post_cnt += 1\n",
    "```\n",
    "We will now save the data to a pandas dataframe, and save the data to an execl file. \n",
    "```python\n",
    "DF = pd.DataFrame(df,columns = ['title','selftext','No. of Comments', 'No. of DACs', 'No. of Deltas', 'No. of Com. with Links','Total Links','Links', 'No. of DAC with Links','No. of Com. using stat. lang.','No. of DACs using stat. lang.','Post Time'])\n",
    "DF.to_excel('data/'+date+'/'+posts+'_'+date+'_CMV.xlsx')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
